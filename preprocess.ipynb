{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.backend import sox_io_backend\n",
    "import shutil\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.sox_effects import apply_effects_tensor\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_snr(waveform, n_fft=1024, hop_length=512, noise_frames=5):\n",
    "    \"\"\"\n",
    "    Computes an approximate SNR (in dB) for the given waveform.\n",
    "    \n",
    "    Args:\n",
    "        waveform (Tensor): Audio tensor of shape [channels, samples]. Should be mono.\n",
    "        n_fft (int): FFT window size.\n",
    "        hop_length (int): Hop length for STFT.\n",
    "        noise_frames (int): Number of initial frames to use as noise estimate.\n",
    "    \n",
    "    Returns:\n",
    "        float: Estimated SNR in decibels.\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy array and squeeze extra dimensions to get a 1D array\n",
    "    y = waveform.numpy().squeeze()\n",
    "    \n",
    "    # Compute the STFT of the signal\n",
    "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "    magnitude = np.abs(D)\n",
    "    \n",
    "    # Estimate noise power from the first few frames\n",
    "    noise_power = np.mean(magnitude[:, :noise_frames] ** 2)\n",
    "    \n",
    "    # Compute overall signal power (mean power across all frames)\n",
    "    signal_power = np.mean(magnitude ** 2)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    epsilon = 1e-8\n",
    "    snr = 10 * np.log10(signal_power / (noise_power + epsilon))\n",
    "    return snr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transform=None, processed_dir=None,  snr_threshold=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_dir (str): Directory with raw audio files.\n",
    "            transform (callable, optional): Function to apply preprocessing to the waveform.\n",
    "            processed_dir (str, optional): Directory to save processed audio files.\n",
    "        \"\"\"\n",
    "        self.audio_dir = audio_dir\n",
    "        self.audio_files = [\n",
    "            os.path.join(audio_dir, f)\n",
    "            for f in os.listdir(audio_dir)\n",
    "            if f.lower().endswith(('.wav', '.mp3'))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "        self.processed_dir = processed_dir\n",
    "        self.snr_threshold = snr_threshold\n",
    "        \n",
    "        if self.processed_dir:\n",
    "            os.makedirs(self.processed_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.audio_files[idx]\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(file_path, backend=\"sox\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            broken_dir = os.path.join(os.path.dirname(self.audio_dir), \"audio_broken\")\n",
    "            os.makedirs(broken_dir, exist_ok=True)\n",
    "            filename = os.path.basename(file_path)\n",
    "            shutil.move(file_path, os.path.join(broken_dir, filename))\n",
    "            self.audio_files.remove(file_path)\n",
    "            return None, None, file_path\n",
    "        \n",
    "        snr = compute_snr(waveform)\n",
    "        if snr < self.snr_threshold:\n",
    "            print(f\"File {file_path} has very low SNR: {snr:.2f} dB. Skipping.\")\n",
    "            # Optionally move to a \"too_noisy\" folder\n",
    "            noisy_dir = os.path.join(os.path.dirname(self.audio_dir), \"audio_noisy\")\n",
    "            os.makedirs(noisy_dir, exist_ok=True)\n",
    "            shutil.move(file_path, os.path.join(noisy_dir, os.path.basename(file_path)))\n",
    "            self.audio_files.remove(file_path)\n",
    "            return None, None, file_path\n",
    "        \n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform, sample_rate)\n",
    "            \n",
    "            print(f\"Processed file: {file_path}\")\n",
    "            print(f\"Sample Rate: {sample_rate}\")\n",
    "            print(f\"Waveform shape after transform: {waveform.shape}\")\n",
    "            waveform_2d = waveform.squeeze()\n",
    "            \n",
    "            if waveform_2d.dim() == 1:\n",
    "                waveform_2d = waveform_2d.unsqueeze(0)\n",
    "            \n",
    "            print(f\"Reshaped waveform: {waveform_2d.shape}\")\n",
    "            \n",
    "            if self.processed_dir:\n",
    "                output_path = os.path.join(self.processed_dir, os.path.basename(file_path))\n",
    "                torchaudio.save(output_path, waveform_2d, sample_rate)\n",
    "                print(f\"Saved to: {output_path}\")\n",
    "                \n",
    "            waveform = waveform_2d\n",
    "        \n",
    "        return waveform, sample_rate, file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_subtraction_transform(waveform, sample_rate, n_fft=1024, hop_length=512, noise_frames=5):\n",
    "    \"\"\"\n",
    "    Applies spectral subtraction to reduce stationary noise.\n",
    "    Assumes that the first few frames (noise_frames) contain only noise.\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy array and squeeze extra dimensions\n",
    "    y = waveform.numpy().squeeze()\n",
    "    # Compute STFT\n",
    "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "    magnitude, phase = np.abs(D), np.angle(D)\n",
    "    # Estimate noise magnitude from the first few frames\n",
    "    noise_mag = np.mean(magnitude[:, :noise_frames], axis=1, keepdims=True)\n",
    "    # Subtract noise estimate and clip negative values\n",
    "    subtracted = magnitude - noise_mag\n",
    "    subtracted[subtracted < 0] = 0\n",
    "    # Reconstruct the complex spectrum and invert the STFT\n",
    "    D_clean = subtracted * np.exp(1j * phase)\n",
    "    y_clean = librosa.istft(D_clean, hop_length=hop_length)\n",
    "    # Convert back to a tensor with a channel dimension\n",
    "    return torch.tensor(y_clean).unsqueeze(0)\n",
    "\n",
    "def wiener_filter_transform(waveform, sample_rate, n_fft=1024, hop_length=512, noise_frames=5, beta=0.002):\n",
    "    \"\"\"\n",
    "    Applies a basic Wiener filter to reduce noise.\n",
    "    The gain is computed for each frequency bin based on an estimated SNR.\n",
    "    \"\"\"\n",
    "    y = waveform.numpy().squeeze()\n",
    "    # Compute STFT\n",
    "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "    magnitude, phase = np.abs(D), np.angle(D)\n",
    "    # Estimate noise from the first few frames\n",
    "    noise_mag = np.mean(magnitude[:, :noise_frames], axis=1, keepdims=True)\n",
    "    power_spec = magnitude ** 2\n",
    "    noise_power = noise_mag ** 2\n",
    "    eps = 1e-8\n",
    "    # Compute Wiener gain factor and clip it to a minimum value beta\n",
    "    gain = np.maximum((power_spec - noise_power) / (power_spec + eps), beta)\n",
    "    # Apply gain (take the square root because we're modifying magnitudes)\n",
    "    filtered_D = np.sqrt(gain) * D\n",
    "    y_clean = librosa.istft(filtered_D, hop_length=hop_length)\n",
    "    return torch.tensor(y_clean).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_transform(waveform, sample_rate, target_sample_rate=16000,\n",
    "                         silence_threshold=1.5, min_silence_duration=0.3,\n",
    "                         noise_reduction_method=None):\n",
    "    \"\"\"\n",
    "    Applies comprehensive preprocessing for ASR tasks, including optional noise reduction.\n",
    "    \n",
    "    Args:\n",
    "        waveform (Tensor): Audio tensor.\n",
    "        sample_rate (int): Sample rate of the audio.\n",
    "        target_sample_rate (int): Target sample rate for resampling.\n",
    "        silence_threshold (float): Threshold percentage for silence detection.\n",
    "        min_silence_duration (float): Minimum duration (in seconds) of silence to trim.\n",
    "        noise_reduction_method (str, optional): Choose 'spectral' or 'wiener' to apply that noise reduction method.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Preprocessed waveform.\n",
    "    \"\"\"\n",
    "    # (1) Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # (2) Volume normalization using SoX gain effect\n",
    "    effects = [[\"gain\", \"-n\", \"-3\"]]\n",
    "    try:\n",
    "        waveform, sample_rate = apply_effects_tensor(waveform, sample_rate, effects)\n",
    "    except Exception as e:\n",
    "        print(f\"Volume normalization failed, skipping: {e}\")\n",
    "    \n",
    "    # (3) Trim only leading and trailing silences using librosa.effects.trim\n",
    "    try:\n",
    "        # Convert waveform to a 1D numpy array\n",
    "        audio_np = waveform.squeeze().numpy()\n",
    "        \n",
    "        # Use librosa.effects.trim to remove only the leading and trailing silence.\n",
    "        # The 'top_db' parameter controls what is considered silence.\n",
    "        # Adjust top_db (e.g., 30) to be less aggressive if needed.\n",
    "        trimmed_audio, _ = librosa.effects.trim(audio_np, top_db=30)\n",
    "        \n",
    "        # Convert back to a PyTorch tensor and add the channel dimension back\n",
    "        waveform = torch.tensor(trimmed_audio).unsqueeze(0)\n",
    "        \n",
    "        print(f\"Trimmed waveform shape: {waveform.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Librosa silence trim failed, skipping: {e}\")\n",
    "\n",
    "    # (4) High-pass filter to cut low-frequency noise\n",
    "    try:\n",
    "        effects = [[\"highpass\", \"100\"]]\n",
    "        waveform, sample_rate = apply_effects_tensor(waveform, sample_rate, effects)\n",
    "    except Exception as e:\n",
    "        print(f\"High-pass filter failed, skipping: {e}\")\n",
    "    \n",
    "    # (5) Optional custom noise reduction using our functions\n",
    "    if noise_reduction_method == 'spectral':\n",
    "        waveform = spectral_subtraction_transform(waveform, sample_rate)\n",
    "    elif noise_reduction_method == 'wiener':\n",
    "        waveform = wiener_filter_transform(waveform, sample_rate)\n",
    "    \n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item[0] is not None]\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files:   0%|          | 1/4285 [00:11<13:29:15, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed waveform shape: torch.Size([1, 208896])\n",
      "Processed file: ./audio_raw/f5a009c0-59fe-40af-bea2-b9d70b817e7d_16e2c90c-529c-4829-84af-925a2abd909d.mp3\n",
      "Sample Rate: 24000\n",
      "Waveform shape after transform: torch.Size([1, 208896])\n",
      "Reshaped waveform: torch.Size([1, 208896])\n",
      "Saved to: ./audio_processed/f5a009c0-59fe-40af-bea2-b9d70b817e7d_16e2c90c-529c-4829-84af-925a2abd909d.mp3\n",
      "DataLoader waveform shape: torch.Size([1, 1, 208896])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files:   0%|          | 2/4285 [00:11<5:42:25,  4.80s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed waveform shape: torch.Size([1, 208384])\n",
      "Processed file: ./audio_raw/f41b73ab-fade-40cc-85e7-cc37e6921f0a_a4f93573-e6f3-4bd5-a6ee-f23a48feb9b3.mp3\n",
      "Sample Rate: 24000\n",
      "Waveform shape after transform: torch.Size([1, 208384])\n",
      "Reshaped waveform: torch.Size([1, 208384])\n",
      "Saved to: ./audio_processed/f41b73ab-fade-40cc-85e7-cc37e6921f0a_a4f93573-e6f3-4bd5-a6ee-f23a48feb9b3.mp3\n",
      "DataLoader waveform shape: torch.Size([1, 1, 208384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files:   0%|          | 2/4285 [00:11<7:01:21,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed waveform shape: torch.Size([1, 159232])\n",
      "Processed file: ./audio_raw/ad67da86-2eb9-4892-a28c-b5a08e767c5f_3645f126-2167-4bb4-9e14-aacb88bbd67c.mp3\n",
      "Sample Rate: 24000\n",
      "Waveform shape after transform: torch.Size([1, 159232])\n",
      "Reshaped waveform: torch.Size([1, 159232])\n",
      "Saved to: ./audio_processed/ad67da86-2eb9-4892-a28c-b5a08e767c5f_3645f126-2167-4bb4-9e14-aacb88bbd67c.mp3\n",
      "DataLoader waveform shape: torch.Size([1, 1, 159232])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define directories for raw and processed audio\n",
    "    AUDIO_DIR = \"./audio_raw\"\n",
    "    PROCESSED_DIR = \"./audio_processed\"\n",
    "    \n",
    "    # Example usage: apply spectral subtraction noise reduction.\n",
    "    # To switch to Wiener filtering, set noise_reduction_method='wiener'\n",
    "    dataset = AudioDataset(\n",
    "        audio_dir=AUDIO_DIR, \n",
    "        transform=lambda w, sr: preprocess_transform(w, sr, noise_reduction_method='spectral'),\n",
    "        processed_dir=PROCESSED_DIR,\n",
    "        snr_threshold= 0.2\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    count = 0\n",
    "    for i, (waveform, sample_rate, file_path) in enumerate(tqdm(dataloader, desc=\"Processing audio files\")):\n",
    "        count += 1\n",
    "        if waveform is None:\n",
    "            print(f\"Skipping broken file: {file_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"DataLoader waveform shape: {waveform.shape}\")\n",
    "        if count > 2:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_speech_summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
