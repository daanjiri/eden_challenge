{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.backend import sox_io_backend\n",
    "import shutil\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.sox_effects import apply_effects_tensor\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transform=None, processed_dir=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_dir (str): Directory with raw audio files.\n",
    "            transform (callable, optional): Function to apply preprocessing to the waveform.\n",
    "            processed_dir (str, optional): Directory to save processed audio files.\n",
    "        \"\"\"\n",
    "        self.audio_dir = audio_dir\n",
    "        self.audio_files = [\n",
    "            os.path.join(audio_dir, f)\n",
    "            for f in os.listdir(audio_dir)\n",
    "            if f.lower().endswith(('.wav', '.mp3'))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "        self.processed_dir = processed_dir\n",
    "        \n",
    "        # Create processed directory if specified\n",
    "        if self.processed_dir:\n",
    "            os.makedirs(self.processed_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.audio_files[idx]\n",
    "        try:\n",
    "            # Load the audio using sox_io backend\n",
    "            waveform, sample_rate = torchaudio.load(file_path, backend=\"sox\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            # Create audio_broken directory if it doesn't exist\n",
    "            broken_dir = os.path.join(os.path.dirname(self.audio_dir), \"audio_broken\")\n",
    "            os.makedirs(broken_dir, exist_ok=True)\n",
    "            \n",
    "            # Move the broken file to audio_broken folder\n",
    "            filename = os.path.basename(file_path)\n",
    "            shutil.move(file_path, os.path.join(broken_dir, filename))\n",
    "            \n",
    "            # Remove this file from our list to avoid future attempts\n",
    "            self.audio_files.remove(file_path)\n",
    "            \n",
    "            # Return None to indicate this file should be skipped\n",
    "            return None, None, file_path\n",
    "        \n",
    "        # Apply the transform if provided\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform, sample_rate)\n",
    "            \n",
    "            # After transform, verify and fix channel dimensions\n",
    "            print(f\"Processed file: {file_path}\")\n",
    "            print(f\"Sample Rate: {sample_rate}\")\n",
    "            print(f\"Waveform shape after transform: {waveform.shape}\")\n",
    "            \n",
    "            # Reshape waveform to 2D: [channels, samples]\n",
    "            # First squeeze to remove all extra dimensions\n",
    "            waveform_2d = waveform.squeeze()\n",
    "            \n",
    "            # If the tensor becomes 1D after squeezing, add a channel dimension\n",
    "            if waveform_2d.dim() == 1:\n",
    "                waveform_2d = waveform_2d.unsqueeze(0)  # Add channel dimension [1, samples]\n",
    "            \n",
    "            print(f\"Reshaped waveform: {waveform_2d.shape}\")\n",
    "            \n",
    "            # Save the processed audio if output directory is specified\n",
    "            if self.processed_dir:\n",
    "                output_path = os.path.join(self.processed_dir, os.path.basename(file_path))\n",
    "                torchaudio.save(output_path, waveform_2d, sample_rate)\n",
    "                print(f\"Saved to: {output_path}\")\n",
    "                \n",
    "            # Update waveform to the properly shaped version\n",
    "            waveform = waveform_2d\n",
    "        \n",
    "        return waveform, sample_rate, file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_subtraction_transform(waveform, sample_rate, n_fft=1024, hop_length=512, noise_frames=5):\n",
    "    \"\"\"\n",
    "    Applies spectral subtraction to reduce stationary noise.\n",
    "    Assumes that the first few frames (noise_frames) contain only noise.\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy array and squeeze extra dimensions\n",
    "    y = waveform.numpy().squeeze()\n",
    "    # Compute STFT\n",
    "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "    magnitude, phase = np.abs(D), np.angle(D)\n",
    "    # Estimate noise magnitude from the first few frames\n",
    "    noise_mag = np.mean(magnitude[:, :noise_frames], axis=1, keepdims=True)\n",
    "    # Subtract noise estimate and clip negative values\n",
    "    subtracted = magnitude - noise_mag\n",
    "    subtracted[subtracted < 0] = 0\n",
    "    # Reconstruct the complex spectrum and invert the STFT\n",
    "    D_clean = subtracted * np.exp(1j * phase)\n",
    "    y_clean = librosa.istft(D_clean, hop_length=hop_length)\n",
    "    # Convert back to a tensor with a channel dimension\n",
    "    return torch.tensor(y_clean).unsqueeze(0)\n",
    "\n",
    "def wiener_filter_transform(waveform, sample_rate, n_fft=1024, hop_length=512, noise_frames=5, beta=0.002):\n",
    "    \"\"\"\n",
    "    Applies a basic Wiener filter to reduce noise.\n",
    "    The gain is computed for each frequency bin based on an estimated SNR.\n",
    "    \"\"\"\n",
    "    y = waveform.numpy().squeeze()\n",
    "    # Compute STFT\n",
    "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "    magnitude, phase = np.abs(D), np.angle(D)\n",
    "    # Estimate noise from the first few frames\n",
    "    noise_mag = np.mean(magnitude[:, :noise_frames], axis=1, keepdims=True)\n",
    "    power_spec = magnitude ** 2\n",
    "    noise_power = noise_mag ** 2\n",
    "    eps = 1e-8\n",
    "    # Compute Wiener gain factor and clip it to a minimum value beta\n",
    "    gain = np.maximum((power_spec - noise_power) / (power_spec + eps), beta)\n",
    "    # Apply gain (take the square root because we're modifying magnitudes)\n",
    "    filtered_D = np.sqrt(gain) * D\n",
    "    y_clean = librosa.istft(filtered_D, hop_length=hop_length)\n",
    "    return torch.tensor(y_clean).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_transform(waveform, sample_rate, target_sample_rate=16000,\n",
    "                         silence_threshold=0.5, min_silence_duration=0.8,\n",
    "                         noise_reduction_method=None):\n",
    "    \"\"\"\n",
    "    Applies comprehensive preprocessing for ASR tasks, including optional noise reduction.\n",
    "    \n",
    "    Args:\n",
    "        waveform (Tensor): Audio tensor.\n",
    "        sample_rate (int): Sample rate of the audio.\n",
    "        target_sample_rate (int): Target sample rate for resampling.\n",
    "        silence_threshold (float): Threshold percentage for silence detection.\n",
    "        min_silence_duration (float): Minimum duration (in seconds) of silence to trim.\n",
    "        noise_reduction_method (str, optional): Choose 'spectral' or 'wiener' to apply that noise reduction method.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Preprocessed waveform.\n",
    "    \"\"\"\n",
    "    # (1) Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # (2) Volume normalization using SoX gain effect\n",
    "    effects = [[\"gain\", \"-n\", \"-3\"]]\n",
    "    try:\n",
    "        waveform, sample_rate = apply_effects_tensor(waveform, sample_rate, effects)\n",
    "    except Exception as e:\n",
    "        print(f\"Volume normalization failed, skipping: {e}\")\n",
    "    \n",
    "    # (3) Optional noise reduction via SoX (if needed)\n",
    "    # try:\n",
    "    #     effects = [[\"noisered\", \"/dev/null\", \"0.2\"]]\n",
    "    #     waveform, sample_rate = apply_effects_tensor(waveform, sample_rate, effects)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Noise reduction via SoX failed, skipping: {e}\")\n",
    "    \n",
    "    # (4) Remove silence from beginning and end\n",
    "    try:\n",
    "        effects = [[\"silence\", \"1\", \"0.1\", f\"{silence_threshold}%\", \"1\", str(min_silence_duration), f\"{silence_threshold}%\"]]\n",
    "        waveform, sample_rate = apply_effects_tensor(waveform, sample_rate, effects)\n",
    "    except Exception as e:\n",
    "        print(f\"Silence removal failed, skipping: {e}\")\n",
    "    \n",
    "    # (5) High-pass filter to cut low-frequency noise\n",
    "    try:\n",
    "        effects = [[\"highpass\", \"100\"]]\n",
    "        waveform, sample_rate = apply_effects_tensor(waveform, sample_rate, effects)\n",
    "    except Exception as e:\n",
    "        print(f\"High-pass filter failed, skipping: {e}\")\n",
    "    \n",
    "    # (6) Optional custom noise reduction using our functions\n",
    "    if noise_reduction_method == 'spectral':\n",
    "        waveform = spectral_subtraction_transform(waveform, sample_rate)\n",
    "    elif noise_reduction_method == 'wiener':\n",
    "        waveform = wiener_filter_transform(waveform, sample_rate)\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item[0] is not None]\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files:   0%|          | 0/4282 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise reduction via SoX failed, skipping: Unsupported effect: noisered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files:   0%|          | 1/4282 [00:03<3:41:48,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file: ./audio_raw/7f7c0f0e-97d1-47b7-98f6-796c0ea190af_54c7ab3d-9c89-49c5-83e1-92adc44d4157.mp3\n",
      "Sample Rate: 24000\n",
      "Waveform shape after transform: torch.Size([1, 69120])\n",
      "Reshaped waveform: torch.Size([1, 69120])\n",
      "Saved to: ./audio_processed/7f7c0f0e-97d1-47b7-98f6-796c0ea190af_54c7ab3d-9c89-49c5-83e1-92adc44d4157.mp3\n",
      "DataLoader waveform shape: torch.Size([1, 1, 69120])\n",
      "Noise reduction via SoX failed, skipping: Unsupported effect: noisered\n",
      "Processed file: ./audio_raw/3acaa58c-1c25-4693-8b88-8634b085e21f_9ad4646b-c33f-481a-9bba-5ff51a93e1ee.mp3\n",
      "Sample Rate: 24000\n",
      "Waveform shape after transform: torch.Size([1, 62464])\n",
      "Reshaped waveform: torch.Size([1, 62464])\n",
      "Saved to: ./audio_processed/3acaa58c-1c25-4693-8b88-8634b085e21f_9ad4646b-c33f-481a-9bba-5ff51a93e1ee.mp3\n",
      "DataLoader waveform shape: torch.Size([1, 1, 62464])\n",
      "Noise reduction via SoX failed, skipping: Unsupported effect: noisered\n",
      "Processed file: ./audio_raw/4c2906fa-e77a-486e-bab2-95903b04a648_bf468261-2e0d-4079-ba50-b60d8a69a7bf.mp3\n",
      "Sample Rate: 24000\n",
      "Waveform shape after transform: torch.Size([1, 79872])\n",
      "Reshaped waveform: torch.Size([1, 79872])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files:   0%|          | 2/4282 [00:03<1:57:25,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ./audio_processed/4c2906fa-e77a-486e-bab2-95903b04a648_bf468261-2e0d-4079-ba50-b60d8a69a7bf.mp3\n",
      "DataLoader waveform shape: torch.Size([1, 1, 79872])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define directories for raw and processed audio\n",
    "    AUDIO_DIR = \"./audio_raw\"\n",
    "    PROCESSED_DIR = \"./audio_processed\"\n",
    "    \n",
    "    # Example usage: apply spectral subtraction noise reduction.\n",
    "    # To switch to Wiener filtering, set noise_reduction_method='wiener'\n",
    "    dataset = AudioDataset(\n",
    "        audio_dir=AUDIO_DIR, \n",
    "        transform=lambda w, sr: preprocess_transform(w, sr, noise_reduction_method='spectral'),\n",
    "        processed_dir=PROCESSED_DIR\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    count = 0\n",
    "    for i, (waveform, sample_rate, file_path) in enumerate(tqdm(dataloader, desc=\"Processing audio files\")):\n",
    "        count += 1\n",
    "        if waveform is None:\n",
    "            print(f\"Skipping broken file: {file_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"DataLoader waveform shape: {waveform.shape}\")\n",
    "        if count > 2:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_speech_summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
